import requests
import ollama
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AIProcessor:
    """Niveau 3 - R√©sum√© par IA: Traitement avec LLM local Ollama (S√âCURIS√â)"""
    
    def __init__(self, model: str = 'nous-hermes2:latest', base_url: str = 'http://localhost:11434', max_articles_per_batch=15):
        self.model = model
        self.base_url = base_url
        self.max_articles_per_batch = max_articles_per_batch  
        self._verify_ollama_connection()
        
        logger.info(f"AI Processor initialized with Ollama using model {self.model}")
    
    def _verify_ollama_connection(self):
        """V√©rifie la connexion √† Ollama et le mod√®le"""
        try:
            # V√©rifier si Ollama est accessible
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.model not in model_names:
                    logger.warning(f"Model {self.model} not found. Available models: {model_names}")
                    logger.info("To install the model, run: ollama pull nous-hermes2:latest")
                else:
                    logger.info(f"‚úÖ Ollama connected successfully with {self.model}")
            else:
                logger.error("‚ùå Cannot connect to Ollama. Make sure it's running: ollama serve")
        except Exception as e:
            logger.error(f"‚ùå Ollama connection error: {e}")
            logger.info("Install Ollama: https://ollama.ai/ and run: ollama serve")
    
    def _query_ollama(self, prompt: str, max_tokens: int = 500) -> str:
        """Envoie une requ√™te √† Ollama avec protection"""
        try:
            
            if len(prompt) > 8000:  # Limite de s√©curit√©
                logger.warning(f"Prompt trop long ({len(prompt)} chars), troncature √† 8000")
                prompt = prompt[:8000] + "..."
            
            logger.info(f"Envoi prompt √† Ollama ({len(prompt)} caract√®res)")
            
            response = ollama.chat(
                model=self.model,
                messages=[
                    {
                        'role': 'user',
                        'content': prompt
                    }
                ],
                options={
                    'num_predict': max_tokens,
                    'temperature': 0.3,
                    'top_p': 0.9,
                }
            )
            
            result = response['message']['content']
            logger.info(f"R√©ponse Ollama re√ßue ({len(result)} caract√®res)")
            return result
            
        except Exception as e:
            logger.error(f"Ollama query error: {e}")
            return "Erreur lors de la requ√™te LLM local"
    
    def categorize_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Cat√©gorise les articles avec Ollama (S√âCURIS√â)"""

        if len(articles) > self.max_articles_per_batch:
            logger.warning(f"Trop d'articles ({len(articles)}), limitation √† {self.max_articles_per_batch}")
            articles = articles[:self.max_articles_per_batch]
        
        if len(articles) == 0:
            logger.warning("Aucun article √† cat√©goriser")
            return []
        
        categorized_articles = []

        if len(articles) > 5:
            logger.info(f"Traitement par lots de {len(articles)} articles")
            return self._batch_categorize_articles(articles)
        else:
            logger.info(f"Traitement individuel de {len(articles)} articles")
        
        # Traitement individuel pour les petits nombres
        for i, article in enumerate(articles, 1):
            try:
                logger.info(f"Cat√©gorisation article {i}/{len(articles)}")

                prompt = f"""Cat√©gorise cet article tech en 2-3 mots max:

Titre: {article['titre'][:100]}
R√©sum√©: {article.get('resume_tldr', '')[:200]}

Cat√©gories: AI/IA, Tech, Data, Security, Mobile, Web3, Product, Dev, Design, Business

R√©ponse (juste les cat√©gories):"""
                
                result = self._query_ollama(prompt, max_tokens=30)
                categories = [cat.strip() for cat in result.split(',') if cat.strip()]
                
                # Validation et nettoyage
                valid_categories = [
                    "AI/IA", "Tech", "Data", "Security", "DevOps", 
                    "Mobile", "Web3", "Blockchain", "Product", "Dev", "Design", "Business"
                ]
                
                filtered_categories = [cat for cat in categories if cat in valid_categories]
                if not filtered_categories:
                    filtered_categories = ["Tech"]  # Cat√©gorie par d√©faut
                
                article['categories_ia'] = filtered_categories[:3]  # Max 3 cat√©gories
                categorized_articles.append(article)
                
                logger.info(f"‚úÖ Article {i} cat√©goris√©: {filtered_categories}")
                
            except Exception as e:
                logger.error(f"‚ùå Erreur cat√©gorisation article {i}: {e}")
                article['categories_ia'] = ["Tech"]
                categorized_articles.append(article)
        
        return categorized_articles
    
    def _batch_categorize_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Cat√©gorisation en lot pour optimiser les performances avec Ollama (S√âCURIS√â)"""
        try:
            
            if len(articles) > self.max_articles_per_batch:
                articles = articles[:self.max_articles_per_batch]
                logger.info(f"Limitation √† {len(articles)} articles pour le batch")

            articles_list = []
            for i, article in enumerate(articles):
                # Troncature pour √©viter les prompts trop longs
                title = article['titre'][:80]
                summary = article.get('resume_tldr', '')[:100]
                articles_list.append(f"#{i+1}. {title}")
                if summary:
                    articles_list.append(f"    {summary}")
            
            articles_text = "\n".join(articles_list)

            prompt = f"""Cat√©gorise ces {len(articles)} articles tech.

{articles_text}

Cat√©gories: AI/IA, Tech, Data, Security, Mobile, Web3, Product, Dev, Design, Business

Format r√©ponse exacte:
#1: Tech, AI/IA
#2: Product, Design
...

R√©ponse:"""

            if len(prompt) > 6000:
                # R√©duire le nombre d'articles
                reduced_count = min(8, len(articles))
                logger.warning(f"Prompt trop long, r√©duction √† {reduced_count} articles")
                return self._batch_categorize_articles(articles[:reduced_count])
            
            result = self._query_ollama(prompt, max_tokens=150)
            
            # Parser la r√©ponse
            lines = result.strip().split('\n')
            for i, article in enumerate(articles):
                try:
                    # Chercher la ligne correspondante
                    pattern = f"#{i+1}:"
                    line = next((l for l in lines if l.startswith(pattern)), None)
                    if line:
                        categories_str = line.split(':', 1)[1].strip()
                        categories = [cat.strip() for cat in categories_str.split(',')]
                        article['categories_ia'] = categories[:3]
                        logger.info(f"‚úÖ Article {i+1} cat√©goris√©: {categories}")
                    else:
                        article['categories_ia'] = ["Tech"]
                        logger.warning(f"‚ö†Ô∏è Article {i+1}: cat√©gorie par d√©faut")
                except Exception as e:
                    logger.error(f"‚ùå Erreur parsing article {i+1}: {e}")
                    article['categories_ia'] = ["Tech"]
            
            return articles
            
        except Exception as e:
            logger.error(f"Batch categorization error: {e}")
            # Fallback : cat√©gorisation individuelle avec limite
            limited_articles = articles[:5]  # Maximum 5 en individuel
            logger.info(f"Fallback: traitement individuel de {len(limited_articles)} articles")
            return self.categorize_articles(limited_articles)
    
    def synthesize_articles(self, articles: List[Dict[str, Any]]) -> str:
        """Synth√©tise tous les articles en un r√©sum√© global (S√âCURIS√â)"""
        try:
            
            if len(articles) > 15:
                logger.warning(f"Trop d'articles pour synth√®se ({len(articles)}), limitation √† 15")
                articles = articles[:15]
            
            if len(articles) == 0:
                return "Aucun article √† synth√©tiser."

            articles_summary = []
            for i, article in enumerate(articles[:10], 1):  # Max 10 pour synth√®se
                title = article['titre'][:60]  # Titre tronqu√©
                summary = article.get('resume_tldr', '')[:80]  # R√©sum√© tronqu√©
                categories = ', '.join(article.get('categories_ia', ['Tech'])[:2])  # Max 2 cat√©gories
                
                articles_summary.append(f"{i}. {title}")
                if summary:
                    articles_summary.append(f"   {summary}")
                articles_summary.append(f"   [{categories}]")
            
            articles_text = "\n".join(articles_summary)

            prompt = f"""R√©sum√© ex√©cutif tech du jour ({len(articles)} articles):

{articles_text}

Cr√©e un r√©sum√© en 3 parties courtes:

üîç TENDANCES (2-3 points):
üìä INSIGHTS (1 paragraphe):  
üí° ACTIONS (2 recommandations):

Max 200 mots total."""

            if len(prompt) > 4000:
                # R√©duction drastique si trop long
                articles_mini = []
                for article in articles[:5]:
                    articles_mini.append(f"‚Ä¢ {article['titre'][:40]}")
                
                prompt = f"""R√©sum√© tech ({len(articles)} articles):
{chr(10).join(articles_mini)}

3 tendances + 2 actions en 100 mots max."""
            
            logger.info(f"Synth√®se de {len(articles)} articles (prompt: {len(prompt)} chars)")
            synthesis = self._query_ollama(prompt, max_tokens=300)
            
            if not synthesis or len(synthesis) < 50:
                # Fallback simple
                synthesis = f"""üîç TENDANCES: {len(articles)} articles tech analys√©s
üìä INSIGHTS: Activit√© tech soutenue avec focus sur l'IA et les nouvelles technologies  
üí° ACTIONS: 1) Surveiller les √©volutions IA 2) Evaluer les nouvelles solutions tech"""
            
            logger.info("‚úÖ Synth√®se g√©n√©r√©e avec succ√®s")
            return synthesis
            
        except Exception as e:
            logger.error(f"‚ùå Erreur synth√®se: {e}")
            # Fallback minimal
            return f"""üîç TENDANCES: {len(articles)} articles tech du jour
üìä INSIGHTS: Veille technologique automatis√©e
üí° ACTIONS: Consulter les articles individuels pour plus de d√©tails"""